{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374c9255",
   "metadata": {},
   "source": [
    "# Grid2WS: Spatial and Temporal Validation of Multi-Source Precipitation Products\n",
    "\n",
    "**Author:** Song  \n",
    "**Description:** This notebook processes native-grid footprint geometries (DAYMET, PRISM, gridMET) extracted via Google Earth Engine. It performs exact geometric intersection with watershed boundaries in a projected coordinate system (NAD83 / UTM Zone 17N) to calculate rigorous area-weighted precipitation means. Finally, it validates these spatial products against local meteorological station and rain gauge data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b8abefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory initialized at: /Users/benthosyy/Desktop/NEXRAD/Grid2WS/2_local_processing/Grid2WS_Validation_Output\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 1 â€” Environment Setup & Global Configurations\n",
    "# ====================================================================\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore') # Suppress harmless geopandas overlay warnings\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# A. Scientific Publication Style Settings\n",
    "# --------------------------------------------------------------------\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams.update({\n",
    "    'font.family': 'sans-serif',\n",
    "    'font.sans-serif': ['Arial', 'DejaVu Sans'],\n",
    "    'font.weight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold',\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.dpi': 300\n",
    "})\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# B. Coordinate Reference Systems (CRS) & Global Constants\n",
    "# --------------------------------------------------------------------\n",
    "CRS_UTM = \"EPSG:26917\"  # NAD83 / UTM Zone 17N (For exact area calculations)\n",
    "CRS_WGS = \"EPSG:4326\"   # WGS84 (Default for GEE exports)\n",
    "YEARS = (2017, 2018)\n",
    "START_DATE = f'{YEARS[0]}-01-01'\n",
    "END_DATE = f'{YEARS[1]}-12-31'\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# C. File Paths (User defined)\n",
    "# --------------------------------------------------------------------\n",
    "# Output Directory\n",
    "OUT_DIR = Path(\".\").resolve() / \"Grid2WS_Validation_Output\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory initialized at: {OUT_DIR}\")\n",
    "\n",
    "# Watershed Boundaries (Exported from GEE)\n",
    "BASIN_CA_GEE = r\"/Users/benthosyy/Downloads/P_consistency/drive-download-20260226T002239Z-1-001/GEE_WS_CABR_export_for_CRS_audit.shp\"\n",
    "BASIN_AR_GEE = r\"/Users/benthosyy/Downloads/P_consistency/drive-download-20260226T002239Z-1-001/GEE_WS_ARWD_export_for_CRS_audit.shp\"\n",
    "\n",
    "# Native-Grid Datasets (Footprints and Daily CSVs from GEE)\n",
    "datasets_to_process = {\n",
    "    \"DAYMET\": {\n",
    "        \"shp\": r\"/Users/benthosyy/Downloads/P_consistency/P_nativegrid_pixels_strict/DAYMET_STRICT_nativegrid_pixel_footprints_CA_AR.shp\",\n",
    "        \"csv\": r\"/Users/benthosyy/Downloads/P_consistency/P_nativegrid_pixels_strict/DAYMET_STRICT_nativegrid_pixels_daily_2017_2018.csv\"\n",
    "    },\n",
    "    \"PRISM\": {\n",
    "        \"shp\": r\"/Users/benthosyy/Downloads/P_consistency/P_nativegrid_pixels_strict/PRISM_STRICT_nativegrid_pixel_footprints_CA_AR.shp\",\n",
    "        \"csv\": r\"/Users/benthosyy/Downloads/P_consistency/P_nativegrid_pixels_strict/PRISM_STRICT_nativegrid_pixels_daily_2017_2018.csv\"\n",
    "    },\n",
    "    \"gridMET\": {\n",
    "        \"shp\": r\"/Users/benthosyy/Downloads/P_consistency/P_nativegrid_pixels_strict/GRIDMET_STRICT_nativegrid_pixel_footprints_CA_AR.shp\",\n",
    "        \"csv\": r\"/Users/benthosyy/Downloads/P_consistency/P_nativegrid_pixels_strict/GRIDMET_STRICT_nativegrid_pixels_daily_2017_2018.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Local Rain Gauges and Meteorological Data\n",
    "GAUGE_UTM = {\n",
    "    \"TO\": {\"easting\": 268856.402, \"northing\": 3895148.822},\n",
    "    \"UA\": {\"easting\": 270550.147, \"northing\": 3896352.771}\n",
    "}\n",
    "\n",
    "MET_FILES = {\n",
    "    'CS01_Input': r\"/Users/benthosyy/Desktop/CodeBits/DHSVM-PNNL-2025/TestCase/1203_StormTiming/DHSVM_met_input_CS01_hourly_0411_mean_wind_interpolation.txt\",\n",
    "    'Gauge_TO':   r\"/Users/benthosyy/Desktop/CodeBits/DHSVM-PNNL-2025/TestCase/1203_StormTiming/processed_rain_gauges_step2/DHSVM_met_input_TO_hourly_2017_2018.txt\",\n",
    "    'Gauge_UA':   r\"/Users/benthosyy/Desktop/CodeBits/DHSVM-PNNL-2025/TestCase/1203_StormTiming/processed_rain_gauges_step2/DHSVM_met_input_UA_hourly_2017_2018.txt\"\n",
    "}\n",
    "\n",
    "# Timeseries Visualization Mapping\n",
    "WS_MAPPING = {\n",
    "    'Camp Branch (CA)': {'csv_col': 'CA_P_mm', 'gauge_key': 'Gauge_TO'},\n",
    "    'Arrowwood (AR)':   {'csv_col': 'AR_P_mm', 'gauge_key': 'Gauge_UA'}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134b23d3",
   "metadata": {},
   "source": [
    "---\n",
    "### Spatial Data Initialization\n",
    "All spatial geometries are rigorously reprojected to EPSG:26917 to ensure conservation of mass when calculating geometric intersection areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bbba29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and projecting basin boundaries to UTM 17N...\n",
      "Initializing rain gauge coordinate objects...\n",
      "Spatial basemap initialized.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 2 â€” Spatial Data Initialization: Basins & Rain Gauges\n",
    "# ====================================================================\n",
    "\n",
    "def read_geospatial(path, default_crs=CRS_WGS):\n",
    "    \"\"\"Safely loads spatial data and assigns a default CRS if missing.\"\"\"\n",
    "    gdf = gpd.read_file(path)\n",
    "    if gdf.crs is None:\n",
    "        gdf = gdf.set_crs(default_crs)\n",
    "    return gdf\n",
    "\n",
    "print(\"Loading and projecting basin boundaries to UTM 17N...\")\n",
    "# 1. Read Basin Shapefiles (WGS84 from GEE)\n",
    "ca_basin = read_geospatial(BASIN_CA_GEE, default_crs=CRS_WGS)\n",
    "ar_basin = read_geospatial(BASIN_AR_GEE, default_crs=CRS_WGS)\n",
    "\n",
    "# 2. Reproject Basins to UTM 17N for planar area calculations\n",
    "ca_utm = ca_basin.to_crs(CRS_UTM)\n",
    "ar_utm = ar_basin.to_crs(CRS_UTM)\n",
    "\n",
    "print(\"Initializing rain gauge coordinate objects...\")\n",
    "# 3. Create Rain Gauges GeoDataFrame\n",
    "gauges_utm = gpd.GeoDataFrame(\n",
    "    {\n",
    "        \"name\": [\"TO\", \"UA\"],\n",
    "        \"geometry\": [\n",
    "            gpd.points_from_xy([GAUGE_UTM[\"TO\"][\"easting\"]], [GAUGE_UTM[\"TO\"][\"northing\"]])[0],\n",
    "            gpd.points_from_xy([GAUGE_UTM[\"UA\"][\"easting\"]], [GAUGE_UTM[\"UA\"][\"northing\"]])[0]\n",
    "        ],\n",
    "    },\n",
    "    crs=CRS_UTM\n",
    ")\n",
    "\n",
    "print(\"Spatial basemap initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3581f11e",
   "metadata": {},
   "source": [
    "---\n",
    "### Core Engine: Exact Spatial Intersection\n",
    "This cell contains the primary function to overlay the native grids onto the watersheds, calculate area weights, and generate publication-quality spatial layout figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42de063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================================\n",
    "# Cell 3 â€” Core Processing & Plotting Function (Perfect Layout & Offset)\n",
    "# ====================================================================\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 0. Plotting Style Constants\n",
    "# --------------------------------------------------------------------\n",
    "CA_COLOR = \"#D62728\"\n",
    "AR_COLOR = \"#1F77B4\"\n",
    "GRID_EDGE = \"0.35\"\n",
    "GRID_LW = 0.9\n",
    "FP_ALPHA = 0.95\n",
    "BASIN_LW = 2.2\n",
    "GAUGE_MS = 100     \n",
    "LABEL_FS = 8.5     \n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# A. Plotting Helper Functions\n",
    "# --------------------------------------------------------------------\n",
    "def fmt_km(x, pos): return f\"{int(round(x / 1000))}\"\n",
    "\n",
    "def add_north_arrow(ax, x=0.95, y=0.90, size=0.10):\n",
    "    ax.annotate(\"N\", xy=(x, y), xytext=(x, y - size), xycoords=\"axes fraction\", textcoords=\"axes fraction\",\n",
    "                ha=\"center\", va=\"center\", fontsize=11, fontweight=\"bold\", arrowprops=dict(arrowstyle=\"-|>\", lw=1.1, color=\"black\"))\n",
    "\n",
    "def add_scale_bar(ax, length_m=2000, location=(0.04, 0.06), lw=2.8):\n",
    "    xmin, xmax, ymin, ymax = ax.get_xlim()[0], ax.get_xlim()[1], ax.get_ylim()[0], ax.get_ylim()[1]\n",
    "    x0, y0 = xmin + location[0] * (xmax - xmin), ymin + location[1] * (ymax - ymin)\n",
    "    ax.plot([x0, x0 + length_m], [y0, y0], color=\"black\", lw=lw, solid_capstyle=\"butt\", zorder=30)\n",
    "    ax.text(x0 + length_m/2, y0 + 0.02*(ymax-ymin), f\"{int(length_m/1000)} km\", ha=\"center\", va=\"bottom\", fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "def set_axis_utm(ax, show_xlabel=True, show_ylabel=True):\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    ax.grid(True, linestyle=\":\", alpha=0.25)\n",
    "    ax.xaxis.set_major_formatter(FuncFormatter(fmt_km))\n",
    "    ax.yaxis.set_major_formatter(FuncFormatter(fmt_km))\n",
    "    \n",
    "    if show_xlabel: ax.set_xlabel(\"Easting (km, NAD83 / UTM 17N)\", fontsize=12, fontweight=\"bold\", labelpad=6)\n",
    "    else: ax.set_xticklabels([])\n",
    "    if show_ylabel: ax.set_ylabel(\"Northing (km, NAD83 / UTM 17N)\", fontsize=12, fontweight=\"bold\", labelpad=6)\n",
    "    else: ax.set_yticklabels([])\n",
    "    \n",
    "    for lab in ax.get_xticklabels(): lab.set_rotation(0)\n",
    "    for lab in ax.get_xticklabels(): lab.set_ha(\"center\")\n",
    "\n",
    "# target_aspect -> 1.75 -> 13.0x11.5\n",
    "def get_padded_extent_fixed_aspect(*gdfs, target_aspect=1.75, pad_frac=0.08): \n",
    "    b = np.vstack([g.total_bounds for g in gdfs])\n",
    "    minx, miny = b[:, 0].min(), b[:, 1].min()\n",
    "    maxx, maxy = b[:, 2].max(), b[:, 3].max()\n",
    "    \n",
    "    dx, dy = maxx - minx, maxy - miny\n",
    "    minx, maxx = minx - dx * pad_frac, maxx + dx * pad_frac\n",
    "    miny, maxy = miny - dy * pad_frac, maxy + dy * pad_frac\n",
    "    \n",
    "    w, h = maxx - minx, maxy - miny\n",
    "    current_aspect = w / h\n",
    "    \n",
    "    if current_aspect < target_aspect:\n",
    "        new_w = h * target_aspect\n",
    "        pad_x = (new_w - w) / 2\n",
    "        minx, maxx = minx - pad_x, maxx + pad_x\n",
    "    elif current_aspect > target_aspect:\n",
    "        new_h = w / target_aspect\n",
    "        pad_y = (new_h - h) / 2\n",
    "        miny, maxy = miny - pad_y, maxy + pad_y\n",
    "        \n",
    "    return (minx, miny, maxx, maxy)\n",
    "\n",
    "def add_pixel_labels(ax, fp, col):\n",
    "    placed = []\n",
    "    min_sep = 120.0\n",
    "    for _, r in fp.iterrows():\n",
    "        val = r.get(col)\n",
    "        if pd.isna(val): continue\n",
    "        pt = r.geometry.representative_point()\n",
    "        x, y = pt.x, pt.y\n",
    "        for (px, py) in placed:\n",
    "            if (x - px)**2 + (y - py)**2 < (min_sep**2): x += 0.5 * min_sep; y += 0.5 * min_sep\n",
    "        ax.text(x, y, f\"{int(round(val))}\", ha=\"center\", va=\"center\", fontsize=LABEL_FS, fontweight=\"bold\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.05\", facecolor=\"white\", alpha=0.75, edgecolor=\"none\"), zorder=20) \n",
    "        placed.append((x, y))\n",
    "\n",
    "def annotate_intersection_percents(ax, inter_gdf, top_n=None):\n",
    "    g = inter_gdf.dropna(subset=[\"w_frac\"]).copy()\n",
    "    g = g[g[\"w_frac\"] > 0].copy()\n",
    "    if len(g) == 0: return\n",
    "    if top_n is not None: g = g.sort_values(\"w_frac\", ascending=False).head(int(top_n))\n",
    "    for _, r in g.iterrows():\n",
    "        pt = r.geometry.representative_point()\n",
    "        if r['w_frac'] >= 0.01: \n",
    "            ax.text(pt.x, pt.y, f\"{100*r['w_frac']:.1f}%\", ha=\"center\", va=\"center\", fontsize=7.5, fontweight=\"bold\",\n",
    "                    bbox=dict(boxstyle=\"round,pad=0.08\", facecolor=\"white\", alpha=0.85, edgecolor=\"none\"), zorder=25)\n",
    "\n",
    "def area_weighted_mean_from_inter(inter: gpd.GeoDataFrame, value_col: str) -> float:\n",
    "    g = inter.dropna(subset=[value_col]).copy()\n",
    "    if len(g) == 0: return np.nan\n",
    "    w = g.geometry.area.values\n",
    "    v = g[value_col].values\n",
    "    if np.sum(w) <= 0: return np.nan\n",
    "    return float(np.sum(w * v) / np.sum(w))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# B. Main Processing & Plotting Loop Function\n",
    "# --------------------------------------------------------------------\n",
    "def process_and_plot_product(product_name, fp_path, csv_path, ca_utm, ar_utm, gauges_utm, out_dir):\n",
    "    print(f\"--- Processing {product_name} ---\")\n",
    "    \n",
    "    fp = gpd.read_file(fp_path)\n",
    "    if fp.crs is None: fp = fp.set_crs(CRS_WGS)\n",
    "    fp_utm = fp.to_crs(CRS_UTM)\n",
    "    fp_utm[\"geometry\"] = fp_utm.geometry.buffer(0) \n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df[\"Year\"] = df[\"Date\"].dt.year\n",
    "    df = df[df[\"Year\"].isin(YEARS)].copy()\n",
    "    df_ann = df.groupby([\"pixel_id\", \"Year\"], as_index=False)[\"P_mm\"].sum().rename(columns={\"P_mm\": \"P_annual_mm\"})\n",
    "    \n",
    "    df_wide = df_ann.pivot(index=\"pixel_id\", columns=\"Year\", values=\"P_annual_mm\").reset_index()\n",
    "    y0, y1 = YEARS\n",
    "    col0, col1 = f\"P{y0}\", f\"P{y1}\"\n",
    "    df_wide.rename(columns={y0: col0, y1: col1}, inplace=True)\n",
    "    \n",
    "    fp_ann_utm = fp_utm.merge(df_wide, on=\"pixel_id\", how=\"left\")\n",
    "    \n",
    "    ca_basin_use = ca_utm[[\"geometry\"]].copy()\n",
    "    ca_basin_use[\"geometry\"] = ca_basin_use.geometry.buffer(0)\n",
    "    ar_basin_use = ar_utm[[\"geometry\"]].copy()\n",
    "    ar_basin_use[\"geometry\"] = ar_basin_use.geometry.buffer(0)\n",
    "    \n",
    "    ca_inter = gpd.overlay(fp_ann_utm, ca_basin_use, how=\"intersection\", keep_geom_type=True)\n",
    "    ar_inter = gpd.overlay(fp_ann_utm, ar_basin_use, how=\"intersection\", keep_geom_type=True)\n",
    "    \n",
    "    ca_inter[\"w_frac\"] = ca_inter.geometry.area / ca_utm.geometry.area.sum()\n",
    "    ar_inter[\"w_frac\"] = ar_inter.geometry.area / ar_utm.geometry.area.sum()\n",
    "\n",
    "    ca_mean_y0 = area_weighted_mean_from_inter(ca_inter, col0)\n",
    "    ca_mean_y1 = area_weighted_mean_from_inter(ca_inter, col1)\n",
    "    ar_mean_y0 = area_weighted_mean_from_inter(ar_inter, col0)\n",
    "    ar_mean_y1 = area_weighted_mean_from_inter(ar_inter, col1)\n",
    "\n",
    "    vals = np.concatenate([fp_ann_utm[col0].dropna().values, fp_ann_utm[col1].dropna().values])\n",
    "    vmin, vmax = float(np.min(vals)), float(np.max(vals))\n",
    "    \n",
    "    xmin, ymin, xmax, ymax = get_padded_extent_fixed_aspect(fp_ann_utm, ca_utm, ar_utm, target_aspect=1.75, pad_frac=0.08)\n",
    "    \n",
    "    ca_pt = ca_utm.geometry.iloc[0].representative_point()\n",
    "    ar_pt = ar_utm.geometry.iloc[0].representative_point()\n",
    "\n",
    "    fig = plt.figure(figsize=(13.0, 11.5))\n",
    "    gs = fig.add_gridspec(nrows=3, ncols=2, height_ratios=[1, 1, 1], width_ratios=[1.0, 1.0], hspace=0.18, wspace=0.03)\n",
    "\n",
    "    ax11, ax12 = fig.add_subplot(gs[0, 0]), fig.add_subplot(gs[0, 1])\n",
    "    ax21, ax22 = fig.add_subplot(gs[1, 0]), fig.add_subplot(gs[1, 1])\n",
    "    ax31, axL  = fig.add_subplot(gs[2, 0]), fig.add_subplot(gs[2, 1])\n",
    "\n",
    "    def draw_pixel(ax, year_col, year_label, show_xlabel, show_ylabel):\n",
    "        fp_ann_utm.plot(ax=ax, column=year_col, cmap=\"viridis\", vmin=vmin, vmax=vmax, edgecolor=GRID_EDGE, linewidth=GRID_LW, alpha=FP_ALPHA, zorder=1)\n",
    "        ca_utm.boundary.plot(ax=ax, color=CA_COLOR, linewidth=BASIN_LW, zorder=5)\n",
    "        ar_utm.boundary.plot(ax=ax, color=AR_COLOR, linewidth=BASIN_LW, zorder=5)\n",
    "        gauges_utm[gauges_utm[\"name\"]==\"TO\"].plot(ax=ax, marker=\"^\", color=\"gold\", edgecolor=\"black\", markersize=GAUGE_MS, zorder=10)\n",
    "        gauges_utm[gauges_utm[\"name\"]==\"UA\"].plot(ax=ax, marker=\"^\", color=\"cyan\", edgecolor=\"black\", markersize=GAUGE_MS, zorder=10)\n",
    "        add_pixel_labels(ax, fp_ann_utm, year_col)\n",
    "        ax.set_title(f\"Pixel-scale annual P ({year_label})\", fontsize=13, fontweight=\"bold\", pad=8)\n",
    "        set_axis_utm(ax, show_xlabel=show_xlabel, show_ylabel=show_ylabel)\n",
    "        ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\n",
    "        add_north_arrow(ax); \n",
    "        if show_xlabel: add_scale_bar(ax, 2000)\n",
    "\n",
    "    draw_pixel(ax11, col0, y0, show_xlabel=False, show_ylabel=True)\n",
    "    draw_pixel(ax12, col1, y1, show_xlabel=False, show_ylabel=False)\n",
    "\n",
    "    def draw_basinmean(ax, year, val_ca, val_ar, show_xlabel, show_ylabel):\n",
    "        fp_ann_utm.plot(ax=ax, facecolor=\"0.92\", edgecolor=GRID_EDGE, linewidth=0.8, alpha=0.55, zorder=1)\n",
    "        ca_inter.plot(ax=ax, color=CA_COLOR, alpha=0.22, edgecolor=\"none\", zorder=3)\n",
    "        ar_inter.plot(ax=ax, color=AR_COLOR, alpha=0.22, edgecolor=\"none\", zorder=3)\n",
    "        ca_utm.boundary.plot(ax=ax, color=CA_COLOR, linewidth=BASIN_LW, zorder=5)\n",
    "        ar_utm.boundary.plot(ax=ax, color=AR_COLOR, linewidth=BASIN_LW, zorder=5)\n",
    "        gauges_utm[gauges_utm[\"name\"]==\"TO\"].plot(ax=ax, marker=\"^\", color=\"gold\", edgecolor=\"black\", markersize=GAUGE_MS, zorder=10)\n",
    "        gauges_utm[gauges_utm[\"name\"]==\"UA\"].plot(ax=ax, marker=\"^\", color=\"cyan\", edgecolor=\"black\", markersize=GAUGE_MS, zorder=10)\n",
    "\n",
    "        ctx, cty = ca_pt.x - 200, ca_pt.y - 1700\n",
    "        atx, aty = ar_pt.x + 200, ar_pt.y + 1500\n",
    "\n",
    "        ax.text(ctx, cty, f\"CA: {val_ca:.0f} mm\", ha=\"center\", va=\"center\", fontsize=9.5, fontweight=\"bold\", bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.9, edgecolor=CA_COLOR, lw=1.5), zorder=30)\n",
    "        ax.text(atx, aty, f\"AR: {val_ar:.0f} mm\", ha=\"center\", va=\"center\", fontsize=9.5, fontweight=\"bold\", bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"white\", alpha=0.9, edgecolor=AR_COLOR, lw=1.5), zorder=30)\n",
    "        \n",
    "        ax.set_title(f\"Area-weighted basin mean, {year}\", fontsize=13, fontweight=\"bold\", pad=8)\n",
    "        set_axis_utm(ax, show_xlabel=show_xlabel, show_ylabel=show_ylabel)\n",
    "        ax.set_xlim(xmin, xmax); ax.set_ylim(ymin, ymax)\n",
    "        add_north_arrow(ax); \n",
    "        if show_xlabel: add_scale_bar(ax, 2000)\n",
    "\n",
    "    draw_basinmean(ax21, y0, ca_mean_y0, ar_mean_y0, show_xlabel=False, show_ylabel=True)\n",
    "    draw_basinmean(ax22, y1, ca_mean_y1, ar_mean_y1, show_xlabel=True, show_ylabel=False) \n",
    "\n",
    "    fp_ann_utm.plot(ax=ax31, facecolor=\"0.92\", edgecolor=GRID_EDGE, linewidth=0.8, alpha=0.55, zorder=1)\n",
    "    ca_inter.plot(ax=ax31, color=CA_COLOR, alpha=0.22, edgecolor=\"none\", zorder=3)\n",
    "    ar_inter.plot(ax=ax31, color=AR_COLOR, alpha=0.22, edgecolor=\"none\", zorder=3)\n",
    "    ca_utm.boundary.plot(ax=ax31, color=CA_COLOR, linewidth=BASIN_LW, zorder=5)\n",
    "    ar_utm.boundary.plot(ax=ax31, color=AR_COLOR, linewidth=BASIN_LW, zorder=5)\n",
    "    gauges_utm[gauges_utm[\"name\"]==\"TO\"].plot(ax=ax31, marker=\"^\", color=\"gold\", edgecolor=\"black\", markersize=GAUGE_MS, zorder=10)\n",
    "    gauges_utm[gauges_utm[\"name\"]==\"UA\"].plot(ax=ax31, marker=\"^\", color=\"cyan\", edgecolor=\"black\", markersize=GAUGE_MS, zorder=10)\n",
    "    \n",
    "    annotate_intersection_percents(ax31, ca_inter, top_n=4)\n",
    "    annotate_intersection_percents(ax31, ar_inter, top_n=4)\n",
    "    ax31.set_title(\"Intersection-area weights schematic\", fontsize=13, fontweight=\"bold\", pad=8)\n",
    "    set_axis_utm(ax31, show_xlabel=True, show_ylabel=True)\n",
    "    ax31.set_xlim(xmin, xmax); ax31.set_ylim(ymin, ymax)\n",
    "    add_north_arrow(ax31); add_scale_bar(ax31, 2000)\n",
    "\n",
    "    axL.axis(\"off\")\n",
    "    handles = [\n",
    "        Line2D([0], [0], color=CA_COLOR, lw=BASIN_LW, label=\"Camp Branch (CA) boundary\"),\n",
    "        Line2D([0], [0], color=AR_COLOR, lw=BASIN_LW, label=\"Arrowwood (AR) boundary\"),\n",
    "        Patch(facecolor=\"0.92\", edgecolor=GRID_EDGE, label=\"Native-grid footprints\"),\n",
    "        Patch(facecolor=CA_COLOR, edgecolor=\"none\", alpha=0.22, label=\"Intersection area (CA $\\\\cap$ footprints)\"),\n",
    "        Patch(facecolor=AR_COLOR, edgecolor=\"none\", alpha=0.22, label=\"Intersection area (AR $\\\\cap$ footprints)\"),\n",
    "        Line2D([0], [0], marker=\"^\", color=\"w\", markerfacecolor=\"gold\", markeredgecolor=\"black\", markersize=10, label=\"TO rain gauge\"),\n",
    "        Line2D([0], [0], marker=\"^\", color=\"w\", markerfacecolor=\"cyan\", markeredgecolor=\"black\", markersize=10, label=\"UA rain gauge\"),\n",
    "    ]\n",
    "    axL.legend(handles=handles, loc=\"upper center\", bbox_to_anchor=(0.5, 0.95), frameon=False, fontsize=11.5)\n",
    "    \n",
    "    cax = axL.inset_axes([0.15, 0.15, 0.70, 0.05]) \n",
    "    sm = plt.cm.ScalarMappable(cmap=\"viridis\", norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    sm._A = []\n",
    "    cb = fig.colorbar(sm, cax=cax, orientation=\"horizontal\", extend=\"both\", extendrect=False)\n",
    "    cb.set_label(\"Annual precipitation (mm)\", fontsize=12, fontweight=\"bold\", labelpad=8)\n",
    "    cb.ax.tick_params(labelsize=10)\n",
    "\n",
    "    fig.suptitle(f\"{product_name} annual precipitation and basin area-weighting (UTM; {y0}â€“{y1})\", fontsize=17, fontweight=\"bold\", y=0.95)\n",
    "    \n",
    "    png_path = out_dir / f\"{product_name}_layoutB_UTM_{y0}_{y1}.png\"\n",
    "    pdf_path = out_dir / f\"{product_name}_layoutB_UTM_{y0}_{y1}.pdf\"\n",
    "    fig.savefig(png_path, dpi=300, bbox_inches=\"tight\")\n",
    "    fig.savefig(pdf_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"[{product_name}] Finished and saved to {png_path.name}!\\n\")\n",
    "    \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80814e8",
   "metadata": {},
   "source": [
    "---\n",
    "### Batch Execution: Spatial Footprint Mapping\n",
    "Executes the rendering loop to generate spatial validation plots for all provided gridded products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dff2a9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing DAYMET ---\n",
      "[DAYMET] Finished and saved to DAYMET_layoutB_UTM_2017_2018.png!\n",
      "\n",
      "--- Processing PRISM ---\n",
      "[PRISM] Finished and saved to PRISM_layoutB_UTM_2017_2018.png!\n",
      "\n",
      "--- Processing gridMET ---\n",
      "[gridMET] Finished and saved to gridMET_layoutB_UTM_2017_2018.png!\n",
      "\n",
      "ðŸŽ‰ Spatial mapping successfully finished!\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 4 â€” Batch Execution: Spatial Footprint Mapping\n",
    "# ====================================================================\n",
    "\n",
    "for product_name, paths in datasets_to_process.items():\n",
    "    process_and_plot_product(\n",
    "        product_name=product_name,\n",
    "        fp_path=paths[\"shp\"],\n",
    "        csv_path=paths[\"csv\"],\n",
    "        ca_utm=ca_utm,         \n",
    "        ar_utm=ar_utm,         \n",
    "        gauges_utm=gauges_utm, \n",
    "        out_dir=OUT_DIR\n",
    "    )\n",
    "\n",
    "print(\"ðŸŽ‰ Spatial mapping successfully finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a84290",
   "metadata": {},
   "source": [
    "---\n",
    "### Timeseries Extraction: Daily Area-Weighted Precipitation\n",
    "Calculates the rigorous intersection area-weighted daily mean precipitation for each basin. This is essential for temporal modeling and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d4c6294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Calculating and Exporting Daily Area-Weighted Means ---\n",
      "Processing DAYMET...\n",
      "  -> Saved to DAYMET_daily_area_weighted_mean_2017_2018.csv\n",
      "Processing PRISM...\n",
      "  -> Saved to PRISM_daily_area_weighted_mean_2017_2018.csv\n",
      "Processing gridMET...\n",
      "  -> Saved to gridMET_daily_area_weighted_mean_2017_2018.csv\n",
      "\n",
      " All daily timeseries successfully exported!\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 6 â€” Export Daily Area-Weighted Mean Precipitation to CSV\n",
    "# ====================================================================\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "print(\"--- Calculating and Exporting Daily Area-Weighted Means ---\")\n",
    "\n",
    "# Loop through the three datasets defined in Cell 4\n",
    "for product_name, paths in datasets_to_process.items():\n",
    "    print(f\"Processing {product_name}...\")\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # 1. Load geometries and intersect to calculate static weights\n",
    "    # -------------------------------------------------------------\n",
    "    fp = gpd.read_file(paths[\"shp\"])\n",
    "    if fp.crs is None: \n",
    "        fp = fp.set_crs(CRS_WGS)\n",
    "    fp_utm = fp.to_crs(CRS_UTM)\n",
    "    fp_utm[\"geometry\"] = fp_utm.geometry.buffer(0)\n",
    "    \n",
    "    # Prepare basin boundaries\n",
    "    ca_basin_use = ca_utm[[\"geometry\"]].copy()\n",
    "    ca_basin_use[\"geometry\"] = ca_basin_use.geometry.buffer(0)\n",
    "    ar_basin_use = ar_utm[[\"geometry\"]].copy()\n",
    "    ar_basin_use[\"geometry\"] = ar_basin_use.geometry.buffer(0)\n",
    "    \n",
    "    # Intersections\n",
    "    ca_inter = gpd.overlay(fp_utm, ca_basin_use, how=\"intersection\", keep_geom_type=True)\n",
    "    ar_inter = gpd.overlay(fp_utm, ar_basin_use, how=\"intersection\", keep_geom_type=True)\n",
    "    \n",
    "    # Calculate weights (area fractions)\n",
    "    ca_inter[\"w_frac_CA\"] = ca_inter.geometry.area / ca_utm.geometry.area.sum()\n",
    "    ar_inter[\"w_frac_AR\"] = ar_inter.geometry.area / ar_utm.geometry.area.sum()\n",
    "    \n",
    "    # Extract only the weight mapping (pixel_id -> weight)\n",
    "    w_ca = ca_inter[[\"pixel_id\", \"w_frac_CA\"]].copy()\n",
    "    w_ar = ar_inter[[\"pixel_id\", \"w_frac_AR\"]].copy()\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # 2. Load Daily CSV and merge weights\n",
    "    # -------------------------------------------------------------\n",
    "    df = pd.read_csv(paths[\"csv\"])\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    df[\"Year\"] = df[\"Date\"].dt.year\n",
    "    df = df[df[\"Year\"].isin(YEARS)].copy()\n",
    "    \n",
    "    # Merge weights into the daily data\n",
    "    df = df.merge(w_ca, on=\"pixel_id\", how=\"left\")\n",
    "    df = df.merge(w_ar, on=\"pixel_id\", how=\"left\")\n",
    "    \n",
    "    # Fill NaNs with 0 for pixels that don't overlap a specific basin\n",
    "    df[\"w_frac_CA\"] = df[\"w_frac_CA\"].fillna(0)\n",
    "    df[\"w_frac_AR\"] = df[\"w_frac_AR\"].fillna(0)\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # 3. Calculate weighted daily precipitation\n",
    "    # -------------------------------------------------------------\n",
    "    df[\"P_weighted_CA\"] = df[\"P_mm\"] * df[\"w_frac_CA\"]\n",
    "    df[\"P_weighted_AR\"] = df[\"P_mm\"] * df[\"w_frac_AR\"]\n",
    "    \n",
    "    # Group by Date to sum up the weighted values across all pixels\n",
    "    df_daily_basin = df.groupby(\"Date\")[[\"P_weighted_CA\", \"P_weighted_AR\"]].sum().reset_index()\n",
    "    \n",
    "    # Rename columns for clarity in the final CSV\n",
    "    df_daily_basin.rename(columns={\n",
    "        \"P_weighted_CA\": \"CA_P_mm\",\n",
    "        \"P_weighted_AR\": \"AR_P_mm\"\n",
    "    }, inplace=True)\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # 4. Export to CSV\n",
    "    # -------------------------------------------------------------\n",
    "    out_csv = OUT_DIR / f\"{product_name}_daily_area_weighted_mean_{YEARS[0]}_{YEARS[1]}.csv\"\n",
    "    df_daily_basin.to_csv(out_csv, index=False)\n",
    "    print(f\"  -> Saved to {out_csv.name}\")\n",
    "\n",
    "print(\"\\n All daily timeseries successfully exported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2024de",
   "metadata": {},
   "source": [
    "---\n",
    "### Data Integrity Check: Spatial vs. Temporal Cross-Validation\n",
    "Performs a sanity check. Mathematical conservation of mass is proven if the sum of weights strictly equals 1.0, and if the annual sums calculated from daily CSVs perfectly match the spatial outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0a51305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Sanity Check: Area Fractions and Annual Sums ---\n",
      "\n",
      "[DAYMET] Weight Verifications (Expected 1.000000):\n",
      "  CA sum(w_frac) = 1.000000\n",
      "  AR sum(w_frac) = 1.000000\n",
      "[DAYMET] CSV Annual Sum Verifications:\n",
      "  2017 -> CA: 1918 mm | AR: 1756 mm\n",
      "  2018 -> CA: 2435 mm | AR: 2298 mm\n",
      "--------------------------------------------------\n",
      "[PRISM] Weight Verifications (Expected 1.000000):\n",
      "  CA sum(w_frac) = 1.000000\n",
      "  AR sum(w_frac) = 1.000000\n",
      "[PRISM] CSV Annual Sum Verifications:\n",
      "  2017 -> CA: 1784 mm | AR: 1620 mm\n",
      "  2018 -> CA: 2004 mm | AR: 1933 mm\n",
      "--------------------------------------------------\n",
      "[gridMET] Weight Verifications (Expected 1.000000):\n",
      "  CA sum(w_frac) = 1.000000\n",
      "  AR sum(w_frac) = 1.000000\n",
      "[gridMET] CSV Annual Sum Verifications:\n",
      "  2017 -> CA: 1873 mm | AR: 1626 mm\n",
      "  2018 -> CA: 2080 mm | AR: 1925 mm\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 6 â€” Data Integrity Check: Spatial vs. Temporal Cross-Validation\n",
    "# ====================================================================\n",
    "\n",
    "print(\"--- Sanity Check: Area Fractions and Annual Sums ---\\n\")\n",
    "\n",
    "for product_name, paths in datasets_to_process.items():\n",
    "    # Verify Fraction Sums (Spatial accuracy)\n",
    "    fp = gpd.read_file(paths[\"shp\"])\n",
    "    if fp.crs is None: fp = fp.set_crs(CRS_WGS)\n",
    "    fp_utm = fp.to_crs(CRS_UTM)\n",
    "    fp_utm[\"geometry\"] = fp_utm.geometry.buffer(0)\n",
    "    \n",
    "    ca_b = ca_utm[[\"geometry\"]].copy(); ca_b[\"geometry\"] = ca_b.geometry.buffer(0)\n",
    "    ar_b = ar_utm[[\"geometry\"]].copy(); ar_b[\"geometry\"] = ar_b.geometry.buffer(0)\n",
    "    \n",
    "    ca_inter = gpd.overlay(fp_utm, ca_b, how=\"intersection\", keep_geom_type=True)\n",
    "    ar_inter = gpd.overlay(fp_utm, ar_b, how=\"intersection\", keep_geom_type=True)\n",
    "    \n",
    "    ca_w_sum = (ca_inter.geometry.area / ca_utm.geometry.area.sum()).sum()\n",
    "    ar_w_sum = (ar_inter.geometry.area / ar_utm.geometry.area.sum()).sum()\n",
    "    \n",
    "    print(f\"[{product_name}] Weight Verifications (Expected 1.000000):\")\n",
    "    print(f\"  CA sum(w_frac) = {ca_w_sum:.6f}\")\n",
    "    print(f\"  AR sum(w_frac) = {ar_w_sum:.6f}\")\n",
    "    \n",
    "    # Verify Daily to Annual Rollups (Temporal accuracy)\n",
    "    csv_path = OUT_DIR / f\"{product_name}_daily_area_weighted_mean_{YEARS[0]}_{YEARS[1]}.csv\"\n",
    "    if csv_path.exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "        df[\"Year\"] = df[\"Date\"].dt.year\n",
    "        annual_sums = df.groupby(\"Year\")[[\"CA_P_mm\", \"AR_P_mm\"]].sum()\n",
    "        \n",
    "        print(f\"[{product_name}] CSV Annual Sum Verifications:\")\n",
    "        for year in YEARS:\n",
    "            if year in annual_sums.index:\n",
    "                ca_ann = annual_sums.loc[year, \"CA_P_mm\"]\n",
    "                ar_ann = annual_sums.loc[year, \"AR_P_mm\"]\n",
    "                print(f\"  {year} -> CA: {ca_ann:.0f} mm | AR: {ar_ann:.0f} mm\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5834f9d3",
   "metadata": {},
   "source": [
    "---\n",
    "### Validation Analytics: Multi-Source Timeseries Comparison\n",
    "Utilizes the strictly area-weighted CSVs to compare remote sensing/climate products against local hydrological baseline inputs (CS01) and point rain gauges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f5103b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Time-Series Validation Sequence...\n",
      "\n",
      "Processing Timeseries Analytics for: Camp Branch (CA)\n",
      "\n",
      "Processing Timeseries Analytics for: Arrowwood (AR)\n",
      "\n",
      " All timeseries analytics exported successfully to: /Users/benthosyy/Desktop/NEXRAD/Grid2WS/2_local_processing/Grid2WS_Validation_Output\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Cell 7 â€” Validation Analytics: Multi-Source Timeseries Comparison\n",
    "# ====================================================================\n",
    "\n",
    "def load_met_txt(name, filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        if \"CS01\" not in name: print(f\"  [Warning] Local met file not found: {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "    try:\n",
    "        df = pd.read_csv(filepath, sep=r'\\s+', header=None, engine='python')\n",
    "        df['Date'] = pd.to_datetime(df[0], format='%m/%d/%Y-%H:%M')\n",
    "        df['P_mm'] = df[6] * 1000.0\n",
    "        df_daily = df.set_index('Date').resample('D').sum(numeric_only=True).reset_index()\n",
    "        df_daily['Source'] = name \n",
    "        return df_daily[['Date', 'Source', 'P_mm']]\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Reading {name}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_area_weighted_column(filepath, target_col, source_label):\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"  [Error] File not found: {filepath}\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(filepath)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    if target_col in df.columns:\n",
    "        sub = df[['Date', target_col]].copy()\n",
    "        sub.rename(columns={target_col: 'P_mm'}, inplace=True)\n",
    "        sub['Source'] = source_label\n",
    "        return sub\n",
    "    return pd.DataFrame()\n",
    "\n",
    "print(\"Initializing Time-Series Validation Sequence...\")\n",
    "df_cs01 = load_met_txt('CS01_Input', MET_FILES['CS01_Input'])\n",
    "\n",
    "for ws_display_name, config in WS_MAPPING.items():\n",
    "    print(f\"\\nProcessing Timeseries Analytics for: {ws_display_name}\")\n",
    "    data_list = []\n",
    "    \n",
    "    if not df_cs01.empty: data_list.append(df_cs01)\n",
    "        \n",
    "    g_key = config['gauge_key']\n",
    "    if g_key in MET_FILES:\n",
    "        local_name = f\"Local ({g_key.replace('Gauge_', '')})\"\n",
    "        df_gauge = load_met_txt(local_name, MET_FILES[g_key])\n",
    "        if not df_gauge.empty: data_list.append(df_gauge)\n",
    "            \n",
    "    target_col = config['csv_col']\n",
    "    sub_daymet = load_area_weighted_column(OUT_DIR / f\"DAYMET_daily_area_weighted_mean_{YEARS[0]}_{YEARS[1]}.csv\", target_col, 'Daymet')\n",
    "    sub_prism = load_area_weighted_column(OUT_DIR / f\"PRISM_daily_area_weighted_mean_{YEARS[0]}_{YEARS[1]}.csv\", target_col, 'PRISM (Climate)')\n",
    "    sub_gridmet = load_area_weighted_column(OUT_DIR / f\"gridMET_daily_area_weighted_mean_{YEARS[0]}_{YEARS[1]}.csv\", target_col, 'GridMET (Radar)')\n",
    "    \n",
    "    if not sub_daymet.empty: data_list.append(sub_daymet)\n",
    "    if not sub_prism.empty: data_list.append(sub_prism)\n",
    "    if not sub_gridmet.empty: data_list.append(sub_gridmet)\n",
    "        \n",
    "    df_ws = pd.concat(data_list, ignore_index=True)\n",
    "    df_ws = df_ws[(df_ws['Date'] >= START_DATE) & (df_ws['Date'] <= END_DATE)]\n",
    "    df_pivot = df_ws.pivot_table(index='Date', columns='Source', values='P_mm', aggfunc='sum')\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # Visualization\n",
    "    # -------------------------------------------------------------\n",
    "    colors = {\n",
    "        'CS01_Input': 'black',\n",
    "        f\"Local ({config['gauge_key'].replace('Gauge_', '')})\": '#d62728', \n",
    "        'Daymet': '#1f77b4',          \n",
    "        'PRISM (Climate)': '#2ca02c', \n",
    "        'GridMET (Radar)': '#9467bd'  \n",
    "    }\n",
    "    safe_name = ws_display_name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    cols_sorted = sorted(df_pivot.columns, key=lambda x: 0 if 'Daymet' in x else 1 if 'PRISM' in x else 2 if 'GridMET' in x else 3 if 'Local' in x else 4)\n",
    "    \n",
    "    # Plot 1: Cumulative\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    df_cumsum = df_pivot.cumsum()\n",
    "    for col in cols_sorted:\n",
    "        c = colors.get(col, 'gray')\n",
    "        ls, lw = ('-', 3.0) if ('Local' in col or 'CS01' in col) else ('--', 2.0)\n",
    "        final_v = df_cumsum[col].iloc[-1]\n",
    "        plt.plot(df_cumsum.index, df_cumsum[col], label=f\"{col} ({final_v:.0f})\", color=c, linestyle=ls, linewidth=lw, alpha=0.9)\n",
    "    plt.title(f'{ws_display_name}: Cumulative Precipitation (2017-2018)', fontsize=16)\n",
    "    plt.ylabel('Cumulative Precipitation (mm)', fontsize=14)\n",
    "    plt.legend(title='Total (mm)', fontsize=11, loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.savefig(OUT_DIR / f'Cumulative_{safe_name}.pdf')\n",
    "    plt.savefig(OUT_DIR / f'Cumulative_{safe_name}.png')\n",
    "    \n",
    "    # Plot 2: Annual Totals\n",
    "    # Note: using 'YE' for Pandas >= 2.2.0 compatibility\n",
    "    df_annual = df_pivot.resample('YE').sum() \n",
    "    df_melt = df_annual.reset_index().melt(id_vars='Date', var_name='Source', value_name='Total_mm')\n",
    "    df_melt['Year'] = df_melt['Date'].dt.year.astype(str)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.barplot(data=df_melt, x='Year', y='Total_mm', hue='Source', palette=colors)\n",
    "    plt.title(f'{ws_display_name}: Annual Total Precipitation', fontsize=16)\n",
    "    plt.ylabel('Total Precipitation (mm)', fontsize=14)\n",
    "    plt.xlabel('Year', fontsize=14)\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', fontsize=11)\n",
    "    \n",
    "    for p in ax.patches:\n",
    "        height = p.get_height()\n",
    "        if height > 0:\n",
    "            ax.annotate(f'{height:.0f}', (p.get_x() + p.get_width() / 2., height), \n",
    "                        ha='center', va='bottom', fontsize=10, fontweight='bold', \n",
    "                        rotation=0, xytext=(0, 5), textcoords='offset points')\n",
    "    plt.ylim(0, df_melt['Total_mm'].max() * 1.15) \n",
    "    plt.savefig(OUT_DIR / f'Annual_{safe_name}.pdf')\n",
    "    plt.savefig(OUT_DIR / f'Annual_{safe_name}.png')\n",
    "\n",
    "    # Plot 3: Monthly Trends\n",
    "    df_monthly = df_pivot.resample('ME').sum()\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for col in cols_sorted:\n",
    "        c = colors.get(col, 'gray')\n",
    "        ls, lw = ('-', 2.5) if ('Local' in col or 'CS01' in col) else ('--', 1.5)\n",
    "        plt.plot(df_monthly.index, df_monthly[col], label=col, color=c, linestyle=ls, linewidth=lw)\n",
    "    plt.title(f'{ws_display_name}: Monthly Precipitation Trends', fontsize=16)\n",
    "    plt.ylabel('Monthly Precipitation (mm)', fontsize=14)\n",
    "    plt.legend(bbox_to_anchor=(1.01, 1), loc='upper left', fontsize=11)\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.savefig(OUT_DIR / f'Monthly_{safe_name}.pdf')\n",
    "    plt.savefig(OUT_DIR / f'Monthly_{safe_name}.png')\n",
    "    plt.close('all')\n",
    "\n",
    "print(f\"\\n All timeseries analytics exported successfully to: {OUT_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_precip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
